version: '3.8'

services:
  projector-backend:
    build: .
    container_name: projector_nest
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - AI_SERVICE_URL=http://ai-model:5000/generate  # URL для обращения к модели ИИ
    volumes:
      - .:/app
      - /app/node_modules
    # depends_on:
    #   - ai-model

  ai-model:
    platform: linux/amd64
    image: ghcr.io/huggingface/text-generation-inference:1.1.0
    platform: linux/amd64
    container_name: projector_ai
    restart: unless-stopped
    ports:
      - "5001:5000"
    environment:
      - MODEL_ID=TinyLlama/TinyLlama-1.1B-Chat-v1.0  # Очень легкая модель (1.1B параметров)
      - QUANTIZE=bitsandbytes-nf4  # Квантование для уменьшения нагрузки
      - MAX_INPUT_LENGTH=1024
      - MAX_TOTAL_TOKENS=2048
    volumes:
      - ai-model-cache:/data  # Кэширование модели для ускорения последующих запусков
    deploy:
      resources:
        limits:
          cpus: '2'  # Ограничиваем использование CPU
          memory: 4G  # Ограничиваем память
    
  chromium:
    build:
      context: .
      dockerfile: Dockerfile.chromium
    container_name: projector_chromium
    depends_on:
      - projector-backend
    restart: unless-stopped
    environment:
      - DISPLAY=:99

volumes:
  ai-model-cache: